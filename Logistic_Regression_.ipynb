{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "F90RWl5ZOREp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical"
      ],
      "metadata": {
        "id": "VjLrKBuaOLNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.What is Logistic Regression, and how does it differ from Linear Regression?**\n",
        "ans-What Is Logistic Regression?\n",
        "Logistic regression is a classification algorithm used when your target variable is categorical, typically binary (e.g., yes/no, spam/not spam, pass/fail).\n",
        "- Instead of predicting a continuous value, it estimates the probability that a given input belongs to a category.\n",
        "- The output is bound between 0 and 1 thanks to the sigmoid function, which maps any value into a probability space.\n",
        "\n",
        "Sigmoid Function Formula: [ \\sigma(z) = \\frac{1}{1 + e^{-z}} ]\n",
        "\n",
        "Where ( z ) is the linear combination of input features.\n",
        "\n",
        "Core Difference\n",
        "\n",
        "- Linear Regression predicts a continuous numeric outcome (like price, income, or temperature).\n",
        "- Logistic Regression predicts a categorical outcome, usually binary (like â€œyes/noâ€, â€œspam/not spamâ€).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oPV4MI6y6BcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.What is the mathematical equation of Logistic Regression?**\n",
        "\n",
        "ans-\n",
        "* Logistic Regression Equation\n",
        " - Linear component:\n",
        "[ z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n ]This is similar to what you'd see in linear regression â€” it's just a weighted sum of features.\n",
        " - Sigmoid (Logistic) function:\n",
        "[ p = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\dots + \\beta_nx_n)}} ]This transforms the linear output ( z ) into a probability ( p ), wh\n",
        "\n"
      ],
      "metadata": {
        "id": "siWfen0j6BfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Why do we use the Sigmoid function in Logistic Regression?**\n",
        "\n",
        "ans-\n",
        "* Purpose of the Sigmoid Function:\n",
        "\n",
        "The sigmoid transforms any real-valued input into a number between 0 and 1, which makes it perfect for probability estimation. In logistic regression, weâ€™re not just predicting a valueâ€”weâ€™re predicting the likelihood that something belongs to a category (like â€œyesâ€ vs â€œnoâ€).\n",
        "\n",
        "* Mathematical Form\n",
        "[ \\sigma(z) = \\frac{1}{1 + e^{-z}} ]\n",
        "  - Here, ( z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n )\n",
        "  - ( \\sigma(z) ) gives us the probability that the outcome belongs to class 1.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c30TtXiv6Bhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.What is the cost function of Logistic Regression?**\n",
        "\n",
        "ans-Cost Function: Log Loss / Cross-Entropy Loss\n",
        "Hereâ€™s how it looks mathematically for a single training example:\n",
        "[ \\text{Cost} = -\\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right] ]\n",
        "- ( y ): Actual label (either 0 or 1)\n",
        "- ( \\hat{y} ): Predicted probability from the sigmoid function\n",
        "\n",
        "For multiple samples (say ( m ) examples):\n",
        "[ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}{(i)} \\log(\\hat{y}{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] ]\n"
      ],
      "metadata": {
        "id": "rSbtub3Y6BkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.What is Regularization in Logistic Regression? Why is it needed?**\n",
        "\n",
        "ans-What Is Regularization?\n",
        "Regularization is a technique to control model complexity in logistic regression (or any machine learning model). It adds a penalty to the cost function to discourage the model from relying too heavily on large coefficients, which often leads to overfitting.\n",
        "\n",
        "Regularized Cost Function (L2 Example)\n",
        "\n",
        "Hereâ€™s how it modifies the original logistic regression cost:\n",
        "\n",
        "[ J(\\theta) = \\underbrace{-\\frac{1}{m} \\sum_{i=1}{(i)} \\log(\\hat{y}{(i)}) \\log(1 - \\hat{y}{n} \\theta_j^2}_{\\text{Regularization penalty}} ]\n",
        "- ( \\lambda ): Regularization strength (higher â†’ stronger penalty)\n",
        "- ( \\theta_j ): Coefficients of the model (excluding bias ( \\theta_0 ))\n",
        "\n",
        "\n",
        "Why Is It Needed?\n",
        "- Prevents overfitting by discouraging overly complex models that memorize the data\n",
        "-  Improves generalization so the model performs better on unseen data\n",
        "-  Promotes sparsity when using L1 regularization (Lasso), effectively removing irrelevant features\n"
      ],
      "metadata": {
        "id": "7mU0mkIH6Bmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.Explain the difference between Lasso, Ridge, and Elastic Net regression?**\n",
        "\n",
        "ans-Absolutely, RAJ! Hereâ€™s a line-by-line comparison of Ridge, Lasso, and Elastic Net regression techniques to keep things sharp and clear:\n",
        "\n",
        "- Penalty Type:\n",
        " - Ridge uses L2 (squared coefficients)\n",
        " - Lasso uses L1 (absolute coefficients)\n",
        " - Elastic Net combines both L1 and L2 penalties\n",
        "\n",
        "- Penalty Formula:\n",
        " - Ridge: ( \\lambda \\sum \\theta_j^2 )\n",
        " - Lasso: ( \\lambda \\sum \\theta_j )\n",
        " - Elastic Net: ( \\lambda_1 \\sum \\theta_j + \\lambda_2 \\sum \\theta_j^2 )\n",
        "\n",
        "- Effect on Coefficients:\n",
        " - Ridge shrinks them toward zero\n",
        " - Lasso can shrink some to exactly zero\n",
        " - Elastic Net shrinks and selects (some go to zero, others just reduce)\n",
        "\n",
        "- Feature Selection Ability:\n",
        " - Ridge keeps all features\n",
        " - Lasso automatically removes irrelevant ones\n",
        " - Elastic Net balances bothâ€”keeps key ones, discards noise\n",
        "\n",
        "- Multicollinearity Handling:\n",
        " - Ridge handles it well\n",
        " - Lasso may struggle with highly correlated features\n",
        " - Elastic Net is designed to work well in multicollinearity situations\n",
        "\n",
        "- Best Use Case:\n",
        " - Ridge: when all features contribute a bit\n",
        " - Lasso: when only a few are important\n",
        " - Elastic Net: when there are many correlated features and sparsity matters\n",
        "\n"
      ],
      "metadata": {
        "id": "R7TwYmvq6Bo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.When should we use Elastic Net instead of Lasso or Ridge?**\n",
        "\n",
        "ans-When to Choose Elastic Net\n",
        "* High Dimensional Data: When you have more features than observations, Elastic Net shines by balancing complexity and generalization.\n",
        "\n",
        "* Correlated Predictors: Ridge handles multicollinearity well, but doesnâ€™t zero out coefficients. Lasso might randomly select one feature from a group of correlated ones. Elastic Net combines bothâ€”shrinks coefficients while retaining groups of correlated features.\n",
        "\n",
        "* Sparse but Stable Models: If you're aiming for some feature selection (like Lasso) without compromising model stability (like Ridge), Elastic Net offers a smooth compromise.\n",
        "\n",
        "* Balance Between L1 and L2 Penalties: You can fine-tune the mixing parameter l1_ratio to adjust the influence of Lasso vs. Ridge. Useful when neither alone gives satisfying results."
      ],
      "metadata": {
        "id": "kXJH_jjD6BrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.What is the impact of the regularization parameter (Î») in Logistic Regression?**\n",
        "\n",
        "ans-\n",
        "**What Î» Does in Logistic Regression**\n",
        "\n",
        "Whether you're using L1 (Lasso) or L2 (Ridge) regularization, the regularization term is added to the loss function to penalize large coefficients. This helps the model generalize better to unseen data.\n",
        "\n",
        "- Small Î» (close to 0)- The penalty is small, so the model can fit the training data closely\n",
        "- Risk of overfitting increases\n",
        "- Coefficients tend to be larger\n",
        "\n",
        "- Large Î»- The penalty is strong, which forces the coefficients to shrink toward zero\n",
        "- Helps prevent overfitting but might underfit if too large\n",
        "- Model becomes simpler, potentially ignoring some features\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "exl5tiAo6BtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.What are the key assumptions of Logistic Regression.?**\n",
        "\n",
        "ans-**Key Assumptions in Logistic Regression**\n",
        "\n",
        "- Binary Outcome (or Multiclass with Extensions)- Standard logistic regression models a binary dependent variable (e.g. success/failure, yes/no).\n",
        "- For multiclass problems, use Multinomial Logistic Regression.\n",
        "\n",
        "- Independence of Observations- Each observation should be independent of the others.\n",
        "- Violations (like repeated measures on individuals) may require mixed models.\n",
        "\n",
        "- No (or Minimal) Multicollinearity- Predictor variables shouldn't be highly correlated.\n",
        "- You can use Variance Inflation Factor (VIF) to diagnose and handle it.\n",
        "\n",
        "- Linearity of Logit (Not of the Outcome)- The logit transformation of the outcome (i.e. log odds) should have a linear relationship with the predictors.\n",
        "- Try plotting each feature against the log odds, or using Box-Tidwell test.\n",
        "\n",
        "- Large Sample Size- Logistic regression relies on maximum likelihood estimation, which benefits from more data.\n",
        "- Especially important when predictors are numerous or categories are imbalanced.\n",
        "\n",
        "- No Outliers with Strong Influence- Outliers can distort the fit; use standardized residuals, Cookâ€™s distance, or Leverage stats to identify them.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QLc1BR_37f1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.What are some alternatives to Logistic Regression for classification task?**\n",
        "\n",
        "ans-\n",
        "1. Model: Decision Trees\n",
        "\n",
        "Key Strengths: Easy to interpret, handles non-linear boundaries âž¤ You can visualize how decisions are madeâ€”great for understanding rules in your data.\n",
        "\n",
        "Best Use Case: Small datasets, clear decision rules âž¤ Ideal if you want transparency in how predictions are made and your data isnâ€™t too complex.\n",
        "\n",
        "2. Model: Random Forests\n",
        "\n",
        "Key Strengths: Reduces overfitting via ensemble averaging âž¤ Combines many trees to create a more robust model, helping smooth out noisy predictions.\n",
        "\n",
        "Best Use Case: Large feature spaces, noisy data âž¤ Useful when there are many variables and data variabilityâ€”helps generalize better.\n",
        "\n",
        "3. Model: Gradient Boosting (e.g., XGBoost)\n",
        "\n",
        "Key Strengths: High accuracy, handles complex relationships âž¤ Builds models sequentially to focus on tough-to-predict casesâ€”great for squeezing out performance.\n",
        "\n",
        "Best Use Case: Kaggle-style datasets, model competitions âž¤ Dominates when you're optimizing performance in structured datasets.\n",
        "\n",
        "4. Model: K-Nearest Neighbors (KNN)\n",
        "\n",
        "Key Strengths: Simple, non-parametric âž¤ No training phaseâ€”makes predictions by comparing similarity to known data points.\n",
        "\n",
        "Best Use Case: Highly irregular decision boundaries âž¤ Handy when the classes are not easily separable using linear boundaries.\n",
        "\n",
        "5. Model: Support Vector Machines (SVM)\n",
        "\n",
        "Key Strengths: Works well in high-dimensional spaces, robust to overfitting âž¤ Finds the best margin between classes and supports kernels for non-linear data.\n",
        "\n",
        "Best Use Case: Text data, small to medium datasets âž¤ Often excels in document classification or cases with many features and few samples.\n",
        "\n",
        "6. Model: Naive Bayes\n",
        "\n",
        "Key Strengths: Fast, performs well on categorical features âž¤ Assumes feature independence; surprisingly powerful for simple applications.\n",
        "\n",
        "Best Use Case: Text classification, spam filtering âž¤ Despite its simplicity, itâ€™s often the baseline winner in NLP tasks."
      ],
      "metadata": {
        "id": "JoYWAhF87gNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11.What are Classification Evaluation Metrics?**\n",
        "\n",
        "ans-Classification Evaluation Metrics are tools used to measure how well a classification model performsâ€”especially when you're predicting categories like spam vs. not spam, or high vs. low churn risk.\n",
        "\n",
        "Hereâ€™s a breakdown of the key metrics and what they tell you:\n",
        "\n",
        "* 1. Accuracy\n",
        "\n",
        " * What it is: The ratio of correct predictions to total predictions.\n",
        "\n",
        " * Best when: Classes are balanced.\n",
        "\n",
        " * Formula: $$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
        "\n",
        "* 2. Precision\n",
        "\n",
        " * What it is: Of all the items predicted as positive, how many were actually positive?\n",
        "\n",
        " * Use case: Important in cases where false positives are costly (like predicting diseases).\n",
        "\n",
        " * Formula: $$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
        "\n",
        "* 3. Recall (Sensitivity or True Positive Rate)\n",
        "\n",
        " * What it is: Of all actual positives, how many did we correctly predict?\n",
        "\n",
        " * Use case: Vital when missing a positive case is risky (like fraud detection).\n",
        "\n",
        " * Formula: $$ \\text{Recall} = \\frac{TP}{TP + FN} $$\n",
        "\n",
        "* 4. F1 Score\n",
        "\n",
        " * What it is: Harmonic mean of precision and recall.\n",
        "\n",
        " * Use case: Balanced measure when classes are imbalanced.\n",
        "\n",
        " * Formula: $$ \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
        "\n",
        "* 5. ROC-AUC Score (Receiver Operating Characteristic - Area Under Curve)\n",
        "\n",
        " * What it is: Measures how well the model separates the classes.\n",
        "\n",
        " * Use case: Great for comparing multiple models.\n",
        "\n",
        "* 6. Confusion Matrix\n",
        "\n",
        " * What it is: A table showing TP, TN, FP, and FN counts.\n",
        "\n",
        " * Use case: Gives a detailed snapshot of prediction performance."
      ],
      "metadata": {
        "id": "OWA4wutFATLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12.How does class imbalance affect Logistic Regression?**\n",
        "\n",
        "ans-Impact of Class Imbalance\n",
        "\n",
        "* Biased Predictions: The model tends to favor the majority class since it's \"seen\" more during training. You might get high accuracy, but very poor recall on the minority class.\n",
        "\n",
        "* Misleading Accuracy: Say 95% of your samples belong to class A and only 5% to class B. If your model predicts class A every time, itâ€™ll show 95% accuracy but totally miss the minority classâ€”bad news if class B is something critical like fraud detection.\n",
        "\n",
        "* Poor Recall & Precision for Minority Class: Especially harmful when the minority class is what you're trying to detect.\n",
        "\n"
      ],
      "metadata": {
        "id": "50LsYd1MATeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13.What is Hyperparameter Tuning in Logistic Regression?**\n",
        "\n",
        "ans-Hyperparameter Tuning in Logistic Regression means finding the best settings that control how the model learnsâ€”not the parameters learned during training (like coefficients), but the ones you set beforehand to guide the process.\n",
        "\n",
        "**Tuning Strategies**\n",
        "\n",
        "1. Grid Search (GridSearchCV)\n",
        "\n",
        " * Test all combinations of hyperparameters.\n",
        "\n",
        " * Great for thorough exploration, though a bit time-consuming.\n",
        "\n",
        "2. Random Search (RandomizedSearchCV)\n",
        "\n",
        " * Randomly sample combinations.\n",
        "\n",
        " * Faster and good for large search spaces.\n",
        "\n",
        "3. Bayesian Optimization (e.g. Optuna, Hyperopt)\n",
        "\n",
        " * More efficient, smarter exploration.\n",
        "\n",
        " * Learns from past trials to suggest better hyperparameter combos."
      ],
      "metadata": {
        "id": "XEPD1jojAThV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14.What are different solvers in Logistic Regression? Which one should be used?**\n",
        "\n",
        "ans-\n",
        "\n",
        "1. liblinear â†’ Uses coordinate descent optimization. â†’ Supports both L1 and L2 regularization. â†’ Only supports One-vs-Rest for multiclass problems. â†’ Best for small datasets and when L1 regularization is needed.\n",
        "\n",
        "2. lbfgs â†’ Uses a quasi-Newton method (BFGS approximation). â†’ Supports only L2 regularization. â†’ Supports both multinomial and One-vs-Rest for multiclass. â†’ Best for large datasets and multiclass classification.\n",
        "\n",
        "3. newton-cg â†’ Uses the Newton-Raphson method with second-order derivatives. â†’ Supports only L2 regularization. â†’ Supports both multinomial and One-vs-Rest for multiclass. â†’ Best when you want precise optimization and can afford more computation.\n",
        "\n",
        "4. sag â†’ Uses Stochastic Average Gradient descent. â†’ Supports only L2 regularization. â†’ Supports both multinomial and One-vs-Rest for multiclass. â†’ Best for large datasets with sparse features.\n",
        "\n",
        "5. saga â†’ An improved version of SAG that supports more regularization. â†’ Supports L1, L2, and Elastic Net regularization. â†’ Supports both multinomial and One-vs-Rest for multiclass. â†’ Best for large datasets, especially with L1 or Elastic Net.\n",
        "\n",
        "\n",
        "**-Which Solver Should we Use?**\n",
        "-  Use liblinear for binary classification on small, clean datasets.\n",
        "-  Use saga when working with sparse or large-scale data, or if you need L1 regularization.\n",
        "-  Use lbfgs when your data is dense, and you donâ€™t need L1 penalty.\n",
        "-  Use newton-cg if you're doing multiclass classification with many classes.\n",
        "-  Avoid sag if your data isnâ€™t large enoughâ€”its advantage really shines on big datasets.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8LKvIYrkATj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. How is Logistic Regression extended for multiclass classification?**\n",
        "\n",
        "ans-\n",
        "* One-vs-Rest (OvR) â€” Default Strategy\n",
        " * Concept: The model builds one binary classifier per class, treating that class as â€œpositiveâ€ and all others as â€œnegative.â€\n",
        "\n",
        " * Prediction: For a new input, all classifiers are evaluated, and the one with the highest confidence wins.\n",
        "\n",
        " * Pros: Simple and efficient. Works well when classes are not highly imbalanced.\n",
        "\n",
        "* Multinomial Logistic Regression\n",
        " * Concept: Uses a softmax function to compute probabilities across all classes simultaneously.\n",
        "\n",
        " * Prediction: Assigns class with the highest probability.\n",
        "\n",
        " * Pros: Models the mutual exclusivity between classes more accurately.\n",
        "\n",
        " * Solver Required: Use lbfgs, newton-cg, or saga (not liblinear).\n"
      ],
      "metadata": {
        "id": "ZRng2DtNATmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16.What are the advantages and disadvantages of Logistic Regression?**\n",
        "\n",
        "ans-\n",
        "* Advantages of Logistic Regression\n",
        "\n",
        " * Easy to Interpret You get clear insights into how each feature affects the probability of an outcome via coefficients.\n",
        "\n",
        " * Low Computational Cost Fast to trainâ€”even on large datasetsâ€”and doesnâ€™t demand heavy hardware.\n",
        "\n",
        " * Works Well for Linearly Separable Data If the classes can be separated with a straight line (or plane), it performs beautifully.\n",
        "\n",
        " * Probability Output Returns probabilities, which are handy for ranking predictions or applying thresholds.\n",
        "\n",
        " * Regularization Options Supports L1, L2, and ElasticNet penalties to prevent overfitting.\n",
        "\n",
        " * Handles Multiclass Problems Extendable via OvR or multinomial strategies.\n",
        "\n",
        "* Disadvantages of Logistic Regression\n",
        "\n",
        " * Linear Decision Boundary Canâ€™t capture complex relationships unless you engineer nonlinear features or use polynomial terms.\n",
        "\n",
        " * Struggles with Multicollinearity Correlated features can distort interpretations. (Though your growing skills in checking VIFs and using PCA help!)\n",
        "\n",
        " * Sensitive to Outliers Extreme values may bias predictions and coefficients.\n",
        "\n",
        " *  Assumes Independence of Observations Not ideal for time-series or hierarchical data without preprocessing.\n",
        "\n",
        " * Performance Drops with Class Imbalance Unless you apply sampling techniques or tweak class_weight."
      ],
      "metadata": {
        "id": "3Yiuim1XATpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17.What are some use cases of Logistic Regression?**\n",
        "\n",
        "ans-\n",
        "1. Medical Diagnosis\n",
        " * Use: Predicting whether a patient has a disease (e.g. diabetes: yes/no).\n",
        "\n",
        " * Why it fits: Outputs probabilities, allowing threshold adjustments for sensitivity.\n",
        "\n",
        "2. Credit Scoring / Default Prediction\n",
        " * Use: Assessing whether someone is likely to default on a loan.\n",
        "\n",
        " * Why it fits: Transparent coefficients help explain risk factors like income, age, or credit history.\n",
        "\n",
        "3. Spam Detection\n",
        " * Use: Classifying emails as spam or not spam based on text and metadata.\n",
        "\n",
        " * Why it fits: Fast and interpretable, especially for keyword-based features.\n",
        "\n",
        "4. Customer Churn Prediction\n",
        " * Use: Predicting whether a customer is likely to leave a service.\n",
        "\n",
        " * Why it fits: Simple enough to deploy quickly and explain to business teams.\n",
        "\n",
        "5. Clinical Trial Success\n",
        " * Use: Estimating whether a new treatment will succeed based on trial data.\n",
        "\n",
        " * Why it fits: Works well with binary outcomes and small datasets."
      ],
      "metadata": {
        "id": "Fue032B8ATs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18.What is the difference between Softmax Regression and Logistic Regression?**\n",
        "\n",
        "ans-\n",
        "\n",
        "**Logistic Regression: Binary Classifier**\n",
        "\n",
        "* Task: Predicts between two classes (e.g., spam vs. not spam).\n",
        "\n",
        "* Activation Function: Uses the sigmoid function $$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
        "\n",
        "* Output: Single probability (between 0 and 1) for the positive class.\n",
        "\n",
        "* Decision Rule: If probability > threshold (e.g. 0.5), classify as positive.\n",
        "\n",
        "**Softmax Regression: Multiclass Classifier**\n",
        "\n",
        "* Task: Handles more than two classes (e.g., cat vs. dog vs. rabbit).\n",
        "\n",
        "* Activation Function: Uses the softmax function $$ P(y = k) = \\frac{e^{z_k}}{\\sum_{j} e^{z_j}} $$\n",
        "\n",
        "* Output: A probability distribution across all classes.\n",
        "\n",
        "* Decision Rule: Choose the class with the highest probability."
      ],
      "metadata": {
        "id": "IUA7Y77WBO-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**\n",
        "\n",
        "ans\n",
        "\n",
        "* Use OvR When:\n",
        " * You want model simplicity and interpretability.\n",
        "\n",
        " * Your dataset is small or mildly imbalanced.\n",
        "\n",
        " * Youâ€™re using a solver like liblinear and sticking to binary-friendly methods.\n",
        "\n",
        "* Use Softmax When:\n",
        " * Your goal is mutually exclusive class assignment (e.g., predicting one product category).\n",
        "\n",
        " * You care about comparative probabilities between classes.\n",
        "\n",
        " * Your solver supports multinomial loss (e.g. lbfgs, saga) and your dataset is rich."
      ],
      "metadata": {
        "id": "xQL557emBPBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20.How do we interpret coefficients in Logistic Regression?**\n",
        "\n",
        "ans-In Logistic Regression, interpreting coefficients means understanding how each feature influences the odds of the predicted classâ€”just not quite the same way as in linear regression. Here's a step-by-step guide to making sense of them:\n",
        "\n",
        "**What the Coefficients Represent**\n",
        "\n",
        "* Each coefficient\n",
        "ð›½\n",
        "ð‘–\n",
        " shows the change in log-odds of the outcome for a one-unit increase in feature\n",
        "ð‘¥\n",
        "ð‘–\n",
        ", holding other features constant.\n",
        "\n",
        "* The logistic model equation: $$ \\text{log}\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n $$\n",
        "\n",
        "**Convert to Odds Ratio**\n",
        "To make it more intuitive, convert coefficients to odds ratios:\n",
        "\n",
        "* Interpretation:\n",
        "\n",
        " * If\n",
        "OddsÂ Ratio\n",
        ">\n",
        "1\n",
        ": Feature increases odds of positive class.\n",
        "\n",
        " * If OddsÂ Ratio< 1: Feature decreases odds of positive class.\n",
        "\n",
        " * If\n",
        "OddsÂ Ratio\n",
        "=\n",
        "1\n",
        ": No effect."
      ],
      "metadata": {
        "id": "3CVgeuQiBPFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "F90RWl5ZOREp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy**"
      ],
      "metadata": {
        "id": "Uf9LimcZOcwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Logistic Regression Model Accuracy: {accuracy:}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IashxkjhObhA",
        "outputId": "6fb73882-e2c4-4a57-82b0-e1892c61fb77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy**"
      ],
      "metadata": {
        "id": "cSrr1Tj2QRuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target!=2\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Apply Logistic Regression with L1 penalty\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')  # liblinear supports L1\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"L1-Regularized Logistic Regression Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NdbJVD5QOed",
        "outputId": "ef9a1836-22a8-4d55-a2f2-c3a3b40164f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1-Regularized Logistic Regression Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients**"
      ],
      "metadata": {
        "id": "c4kJ3KzKTjli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target != 2\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')  # liblinear supports L2 as well\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"L2-Regularized Logistic Regression Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nModel Coefficients:\")\n",
        "coefficients = pd.Series(model.coef_[0], index=iris.feature_names)\n",
        "print(coefficients)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2ABhcvlTU3k",
        "outputId": "1635806a-4e59-4f87-999f-b6e5791dc291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2-Regularized Logistic Regression Accuracy: 1.00\n",
            "\n",
            "Model Coefficients:\n",
            "sepal length (cm)    1.528642\n",
            "sepal width (cm)     1.432447\n",
            "petal length (cm)   -2.304828\n",
            "petal width (cm)    -2.085845\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')**"
      ],
      "metadata": {
        "id": "e6GYFL9QVtf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,n_redundant=5,random_state=1)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=1)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train Logistic Regression model with Elastic Net\n",
        "model = LogisticRegression(penalty='elasticnet',\n",
        "                           solver='saga',       # Required for 'elasticnet'\n",
        "                           l1_ratio=0.5,        # Balance between L1 & L2 (0 = L2, 1 = L1)\n",
        "                           C=1.0,               # Regularization strength (inverse)\n",
        "                           max_iter=1000,       # Increase if model doesn't converge\n",
        "                           random_state=1)\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "tHoENo4wVpv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50bb88a-e36f-416e-b094-61df2378e6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'**"
      ],
      "metadata": {
        "id": "PzolwbCSCB2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Wrap Logistic Regression in OneVsRestClassifier\n",
        "\n",
        "ovr_model = OneVsRestClassifier(LogisticRegression(solver='lbfgs', max_iter=500, random_state=42))\n",
        "\n",
        "# Train the model\n",
        "ovr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ovr_model.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhasrJTpB-sE",
        "outputId": "bed6ed40-0711-47df-cd1b-9d456b43f6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.90\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       0.89      0.80      0.84        10\n",
            "           2       0.82      0.90      0.86        10\n",
            "\n",
            "    accuracy                           0.90        30\n",
            "   macro avg       0.90      0.90      0.90        30\n",
            "weighted avg       0.90      0.90      0.90        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy**"
      ],
      "metadata": {
        "id": "4lKT2gLeEVeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define parameter grid (split into groups to avoid l1_ratio warning)\n",
        "param_grid = [\n",
        "    {'penalty': ['l1'], 'C': [0.01, 0.1, 1, 10]},\n",
        "    {'penalty': ['l2'], 'C': [0.01, 0.1, 1, 10]},\n",
        "    {'penalty': ['elasticnet'], 'C': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7]}\n",
        "]\n",
        "\n",
        "# Create Logistic Regression model with saga solver\n",
        "model = LogisticRegression(solver='saga', max_iter=1000)\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Output best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "y_pred = grid.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF4lVAcSEU_D",
        "outputId": "536f973c-f3c9-442a-c1ff-e6845aed1e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.1, 'penalty': 'l1'}\n",
            "Test Accuracy: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy**"
      ],
      "metadata": {
        "id": "7qhNkazdHtp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Set up Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize model\n",
        "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "# Perform stratified K-fold cross-validation\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "# Print average accuracy\n",
        "avg_accuracy = np.mean(accuracies)\n",
        "print(f\"Average Accuracy across folds: {avg_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_2ueuU3HngX",
        "outputId": "9225892e-bd99-499b-baa5-13f640d3ae09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy across folds: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy.**"
      ],
      "metadata": {
        "id": "aGBjJKASLz5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load CSV data\n",
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "kgrJVqnfJeOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy?**"
      ],
      "metadata": {
        "id": "nqc1LGyWNtK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define hyperparameter search space\n",
        "param_dist = {\n",
        "    'C': loguniform(0.01, 10),             # Continuous range for regularization strength\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'], # Penalty types\n",
        "    'solver': ['saga'],                    # 'saga' supports all penalties\n",
        "    'l1_ratio': [0.3, 0.5, 0.7]            # Only used for 'elasticnet'\n",
        "}\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
        "                            n_iter=20, cv=5, scoring='accuracy',\n",
        "                            random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the search\n",
        "search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate\n",
        "best_params = search.best_params_\n",
        "y_pred = search.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHm1l1ZXQiMv",
        "outputId": "afa43f23-69e7-4855-a942-a769db670902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': np.float64(0.13292918943162169), 'l1_ratio': 0.3, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "Test Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy**"
      ],
      "metadata": {
        "id": "TBobIiNQQlY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the Logistic Regression base model\n",
        "base_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "\n",
        "# Wrap with One-vs-One strategy\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "\n",
        "# Train the OvO classifier\n",
        "ovo_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ovo_model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"One-vs-One Classification Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je-XY3m_Qi-M",
        "outputId": "3e4079c5-4dd8-4003-d0e3-f344a496c897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Classification Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification**"
      ],
      "metadata": {
        "id": "hBRYHeXUQ-Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "\n",
        "# Step 1: Create synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10,\n",
        "                           n_informative=8, n_redundant=2,\n",
        "                           n_classes=2, random_state=42)\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y)\n",
        "\n",
        "# Step 3: Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Step 6: Compute Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Visualize Confusion Matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title(f'Confusion Matrix (Accuracy = {accuracy:.2f})')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "PN2H2SlGQ8Qx",
        "outputId": "a8ac8a62-403b-4111-f2a1-03934251a71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score**"
      ],
      "metadata": {
        "id": "yPBosZqM2SmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load and scale data\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model with updated parameters\n",
        "model = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")"
      ],
      "metadata": {
        "id": "jM1NsQ8-SBN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1280a91-5542-4661-88ab-41e42ce768be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9907\n",
            "Recall:    0.9815\n",
            "F1-Score:  0.9860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance**"
      ],
      "metadata": {
        "id": "gLQ1mjgk3lzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n",
        "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Scale the data for better convergence\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train logistic regression with balanced class weights\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNEXPPy33lNR",
        "outputId": "4d8d616c-106b-44e4-96a5-aa9e776b8990"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9756    0.8989    0.9357       267\n",
            "           1     0.5000    0.8182    0.6207        33\n",
            "\n",
            "    accuracy                         0.8900       300\n",
            "   macro avg     0.7378    0.8585    0.7782       300\n",
            "weighted avg     0.9233    0.8900    0.9010       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance**"
      ],
      "metadata": {
        "id": "HP-_MeWP4Y8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Titanic dataset from seaborn\n",
        "df = sns.load_dataset(\"titanic\")\n",
        "\n",
        "# Drop rows with missing 'embarked' and 'fare'\n",
        "df = df.dropna(subset=['embarked', 'fare'])\n",
        "\n",
        "# Fill missing 'age' with median\n",
        "df['age'] = df['age'].fillna(df['age'].median())\n",
        "\n",
        "# Encode categorical variables\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "df['class'] = df['class'].map({'First': 1, 'Second': 2, 'Third': 3})\n",
        "\n",
        "# Feature selection\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Handle any remaining missing values\n",
        "X = X.dropna()\n",
        "y = y.loc[X.index]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt3mCMJ14Xb0",
        "outputId": "da485b45-864f-45d3-f2a4-52e52d0df611"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8400    0.7706    0.8038       109\n",
            "           1     0.6795    0.7681    0.7211        69\n",
            "\n",
            "    accuracy                         0.7697       178\n",
            "   macro avg     0.7597    0.7694    0.7625       178\n",
            "weighted avg     0.7778    0.7697    0.7718       178\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling**"
      ],
      "metadata": {
        "id": "Xb9hdjVh57og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# --- Model without scaling ---\n",
        "model_unscaled = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "model_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = model_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# --- Model with scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# --- Results ---\n",
        "print(f\"Accuracy without scaling: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgR3_hZR56Hm",
        "outputId": "2ad06416-caa4-4e1d-8e19-ffaed50eca0e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.9649\n",
            "Accuracy with scaling:    0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score**"
      ],
      "metadata": {
        "id": "_6b17QD36M2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load a sample dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling (recommended for logistic regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kMHrxkS6KE9",
        "outputId": "b43d59f4-66d0-42bc-9505-00fe72501505"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy?**"
      ],
      "metadata": {
        "id": "t5wUHkNV6e5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling for better performance\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with custom C\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3LmLRPA6d0O",
        "outputId": "157fe1e4-dc65-4735-858b-90534bd6b2ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 0.9883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18.Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients**"
      ],
      "metadata": {
        "id": "gVYP4qMB6vXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Load a sample dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features for better convergence\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get feature coefficients\n",
        "coefficients = model.coef_[0]\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "feature_importance['Absolute Importance'] = feature_importance['Coefficient'].abs()\n",
        "feature_importance = feature_importance.sort_values(\n",
        "    by='Absolute Importance', ascending=False\n",
        ")\n",
        "\n",
        "# Display top features\n",
        "print(\"Top Features Influencing Predictions:\")\n",
        "print(feature_importance.head(10).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MJ1bJcl6un-",
        "outputId": "4a2d70ca-7e4e-4a71-f3f2-da602100ccec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Features Influencing Predictions:\n",
            "             Feature  Coefficient  Absolute Importance\n",
            "       worst texture    -1.291134             1.291134\n",
            "        radius error    -1.244185             1.244185\n",
            "      worst symmetry    -1.202497             1.202497\n",
            " mean concave points    -1.103646             1.103646\n",
            "     worst concavity    -0.974874             0.974874\n",
            "          area error    -0.868623             0.868623\n",
            "        worst radius    -0.814530             0.814530\n",
            "worst concave points    -0.808050             0.808050\n",
            "          worst area    -0.774047             0.774047\n",
            "      mean concavity    -0.747674             0.747674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19.Write a Python program to train Logistic Regression and evaluate its performance using Cohenâ€™s Kappa\n",
        "Score**"
      ],
      "metadata": {
        "id": "MmJkhZOw7CBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate with Cohenâ€™s Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohenâ€™s Kappa Score: {kappa_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vdCjR7w7Bmv",
        "outputId": "b51965df-cc27-4c9b-d2be-4c4342783f59"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohenâ€™s Kappa Score: 0.9624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20.Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classification**"
      ],
      "metadata": {
        "id": "tkURKfZc7PUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load sample dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probability scores for the positive class\n",
        "y_scores = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Compute precision-recall pairs\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Visualize the Precision-Recall curve\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"Precision-Recall Curve for Logistic Regression\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Krt8cPgm7O45",
        "outputId": "2871a445-de4e-4f7f-d9f7-ca81467009f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy**"
      ],
      "metadata": {
        "id": "oSqm4VCbU3m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define solvers to test\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate Logistic Regression with each solver\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results[solver] = acc\n",
        "\n",
        "# Display results\n",
        "print(\"Solver Accuracy Comparison:\")\n",
        "for solver, acc in results.items():\n",
        "    print(f\"{solver:<10}: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDSplPo0VF5L",
        "outputId": "77778bcb-07f6-4b8b-a416-261bc45d103c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver Accuracy Comparison:\n",
            "liblinear : 0.9825\n",
            "saga      : 0.9825\n",
            "lbfgs     : 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**22.Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)**"
      ],
      "metadata": {
        "id": "GDfPpAXYU_gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate using Matthews Correlation Coefficient\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnj102xeVGYK",
        "outputId": "80ced4fe-627f-4451-fab5-0a388b3ba392"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.9625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling**\n",
        "\n"
      ],
      "metadata": {
        "id": "_0b-OtrLVFDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# --- Logistic Regression on raw data ---\n",
        "model_raw = LogisticRegression(max_iter=5000, solver='lbfgs')\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# --- Feature scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- Logistic Regression on scaled data ---\n",
        "model_scaled = LogisticRegression(max_iter=5000, solver='lbfgs')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# --- Print comparison ---\n",
        "print(f\"Accuracy on raw data:          {acc_raw:.4f}\")\n",
        "print(f\"Accuracy on standardized data: {acc_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df61v61oVR7L",
        "outputId": "fb4ca84b-e974-4f09-eab8-dc523b1f5e05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data:          0.9766\n",
            "Accuracy on standardized data: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation**"
      ],
      "metadata": {
        "id": "LL33tGHvVQgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define parameter grid for C\n",
        "param_grid = {'C': np.logspace(-3, 3, 10)}\n",
        "\n",
        "# GridSearch with cross-validation\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best parameter and accuracy\n",
        "best_C = grid.best_params_['C']\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Optimal C value: {best_C}\")\n",
        "print(f\"Accuracy with optimal C: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEaKs4g27f_z",
        "outputId": "c6a4b4e4-9b2c-46d9-e0cc-7905746af5b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C value: 2.154434690031882\n",
            "Accuracy with optimal C: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions.**"
      ],
      "metadata": {
        "id": "HOuwf0dsVfbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Save model and scaler using joblib\n",
        "joblib.dump(model, 'logistic_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "print(\" Model and scaler saved.\")\n",
        "\n",
        "# Step 6: Load model and scaler\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "loaded_scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Step 7: Use loaded objects to make predictions\n",
        "X_new_scaled = loaded_scaler.transform(X_test)\n",
        "y_pred = loaded_model.predict(X_new_scaled)\n",
        "\n",
        "# Step 8: Evaluate predictions\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy after reloading: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBOmlNBWVlVP",
        "outputId": "8c4b5263-5806-4d99-c2d4-fa1fc0523f16"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model and scaler saved.\n",
            "Accuracy after reloading: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6PtAOO_DYF7N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}